{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-dimensional Scaling (MDS)\n",
    "\n",
    "Author: Matt Smart\n",
    "\n",
    "[Overview](#linkOverview)  \n",
    "[Details](#linkDetails)  \n",
    "[Algorithm](#linkAlgorithm)  \n",
    "[Example](#linkExample)  \n",
    "[Resources](#linkResources)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview <a id='linkOverview'></a>\n",
    "- Non-linear dimension reduction technique  \n",
    "- Rough idea - given high dimensional data $X$, find a lower dimensional representation $Y$ such that the global distance structure is preserved\n",
    "- Two subtypes of MDS: metric (quantitative) and non-metric (qualitative)\n",
    "- Common \"first resort\" technique, like PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details <a id='linkDetails'></a>\n",
    "\n",
    "#### Metric MDS\n",
    "Setup:\n",
    "- Suppose one has $p$ samples of N-dimensional data points, $x_i\\in\\mathbb{R}^N$\n",
    "- Store these samples columnwise as $X\\in\\mathbb{R}^{N\\,\\times\\,p}$\n",
    "- We call this the original data matrix, or simply the data\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the data space (high dim)\n",
    "- Assumption: there is a meaningful metric (e.g. Euclidean distance) on the latent space (low dim)\n",
    "\n",
    "Goal:\n",
    "- Given N-dim data $X$, a metric $d(\\cdot,\\cdot)$ on $\\mathbb{R}^N$, a target dimension $k<N$, and a metric $g(\\cdot,\\cdot)$ on $\\mathbb{R}^k$\n",
    "- FInd an embedding $Y\\in\\mathbb{R}^{k\\,\\times\\,p}$ (i.e. a $y_i\\in\\mathbb{R}^k$ for each $x_i\\in\\mathbb{R}^N$) such that distances $d_{ij}$, $g_{ij}$ are preserved between representations\n",
    "\n",
    "Objective function: $$Y^\\ast=\\operatorname*{arg\\,max}_Y {\\sum_{i<j}{w_{ij}\\left|d_{ij}\\left(X\\right)-g_{ij}\\left(Y\\right)\\right|}}$$\n",
    "\n",
    "Notes:\n",
    "- Define $f(W,X,Y)\\equiv{\\sum_{i<j}{w_{ij}\\left|d_{ij}\\left(X\\right)-g_{ij}\\left(Y\\right)\\right|}}$, then  $Y^\\ast=\\operatorname*{arg\\,max}_Y f(W,X,Y)$\n",
    "- Use the free weights $w_{ij}$ to specify the confidence (or precision) of $d_{ij}(X)$ measurements\n",
    "- Can one show the objective function monontonically increases as target dimension decreases?\n",
    "- Immediate solution degeneracy: if $Y^\\ast$ is optimal, so is any rotation\n",
    "\n",
    "Limitations:\n",
    "- What would happen if we tried to embed an equilateral triangle in 2D into 1D?\n",
    "\n",
    "\n",
    "#### Non-metric MDS\n",
    "Setup:\n",
    "- One has $p$ objects, $\\{x_i\\}_{i=1}^p$\n",
    "- Assumption: there is a notion of dissimilarity between the objects\n",
    "    - note this is weaker, or more general, than specifying a metric\n",
    "    - e.g. a ranking of dissimilarities may be sufficient, but is clearly weaker than specifiying distance\n",
    "- Assumption: one can construct a $p \\times p$ dissimilarity matrix $D$ from the data\n",
    "\n",
    "Goal: \n",
    "- Preserve ordination of the dissimilarity\n",
    "- E.g. If $d_{12}\\left(X\\right)<d_{13}\\left(X\\right)$, then should have $d_{12}\\left(Y\\right)<d_{13}\\left(Y\\right)$\n",
    "\n",
    "Notes:\n",
    "- mention stress idea\n",
    "\n",
    "#### Interpolating between them\n",
    "Ch4 Cox text p1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm <a id='linkAlgorithm'></a>\n",
    "\n",
    "#### Metric MDS\n",
    "\n",
    "Input:\n",
    "- data $X\\in\\mathbb{R}^{N\\,\\times\\,p}$\n",
    "- an embedding or target dimension $1\\leq k<N$\n",
    "- a high-dim metric $d:\\mathbb{R}^N \\times \\mathbb{R}^N\\to R$\n",
    "- a low-dim metric $d:\\mathbb{R}^k \\times \\mathbb{R}^k\\to R$\n",
    "- optional: upper triangular weight matrix $W$ (default is all $w_{ij}=1$)\n",
    "\n",
    "Initialize step: compute $D$, the $p\\times p$ distance matrix using input data, $d_{ij}=d(x_i,x_j)$\n",
    "\n",
    "Optimization:\n",
    "- solve $Y^\\ast=\\operatorname*{arg\\,max}_Y f(W,X,Y)$ by gradient descent\n",
    "\n",
    "Output:\n",
    "- embedding (k-dim representation) $Y\\in\\mathbb{R}^{k\\,\\times\\,p}$\n",
    "\n",
    "Runtime:\n",
    "- MDS $\\approx O\\left(p^3\\right)$  (where $p$ is the number of $\\mathbb{R}^N$ data points)\n",
    "- compare vs. e.g. PCA $\\approx O(p^2)$\n",
    "\n",
    "#### Non-metric MDS\n",
    "...mention stress idea"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example <a id='linkExample'></a>\n",
    "one or more examples on applicable data  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources <a id='linkResources'></a>\n",
    "- Mehta et al., 2017. A high-bias, low-variance introduction to Machine Learning for physicists. https://arxiv.org/abs/1803.08823\n",
    "- Cox and Cox, 2001. (MDS textbook, see Ch2, Ch3, Ch4)\n",
    "- Borg and Groenen, 2005. (MDS textbook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
